\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{./macros.tex}

% info for header block in upper right hand corner
\name{------}
\class{Math 189r}
\assignment{Homework 3}
\duedate{November 7, 2016}

\begin{document}

There are 5 problems in this set. You need to do 3 problems the first week and 2 the second
week. Instead of a sixth problem, youare encouraged to work on your final project.
Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
When implementing algorithms you may not use any library (such as \texttt{sklearn})
that already implements the algorithms but you may use any other library for
data cleaning and numeric purposes (\texttt{numpy} or \texttt{pandas}). Use common
sense. Problems are in no specific order.\\[1em]

\textbf{1 (Murphy 11.2 - EM for Mixtures of Gaussians)} Show that the M step for ML
estimation of a mixture of Gaussians is given by
\begin{align*}
    \mub_k &= \frac{\sum_i r_{ik}\xx_i}{r_k}\\
    \Sigmab_k &= \frac{1}{r_k}\sum_i r_{ik}(\xx_i - \mub_k)(\xx_i - \mub_k)^\T = \frac{1}{r_k}\sum_i r_{ik}\xx_i\xx_i^\T - r_k\mub_k\mub_k^\T.
\end{align*}
\vspace{15mm}
We have that the likelihood is $$ -\frac{1}{2} \sum_i r_{ik} \left[ \log|\Sigma_k| + (\xx_i - \mub_k)^\T \Sigma_k^{-1} (\xx_i - \mub_k) \right] $$

To find the means, we take $\frac{\partial}{\partial \mub_k} l(\mub_k, \Sigma_k) = 0$.
\begin{align*}
	0 &= \frac{\partial}{\partial \mub_k} -\frac{1}{2} \sum_i r_{ik} \left[ \log|\Sigma_k| + (\xx_i - \mub_k)^\T \Sigma_k^{-1} (\xx_i - \mub_k) \right] \\
	0 &= -\frac{1}{2}  \sum_i r_{ik} \frac{\partial}{\partial \mub_k} (\xx_i - \mub_k)^\T \Sigma_k^{-1} (\xx_i - \mub_k) &\\
	0 &= \sum_i r_{ik} \Sigma_{k}^{-1} (\xx_i - \mub_k) \text{  Matrix cookbook 86}\\
    \Sigma_{k}^{-1} \mub_k \sum_i r_{ik} &= \sum_i r_{ik} \Sigma_{k}^{-1} \xx_i \\
    \mub_k &= \frac{\sum_i r_{ik}\xx_i}{r_k} \text{ Multiplied by $\Sigma_{k}$ on both sides. }
\end{align*} 
Where $r_k =\sum_i r_{ik} $.

To find $\Sigma_k$, we take $\frac{\partial}{\partial \Sigma_k} l(\mub_k, \Sigma_k) = 0$.

\begin{align*}
    0 &= \frac{\partial}{\partial \Sigma_k} -\frac{1}{2} \sum_i r_{ik} \left[ \log|\Sigma_k| + (\xx_i - \mub_k)^\T \Sigma_k^{-1} (\xx_i - \mub_k) \right]\\
    &= \sum_i r_{ik} \left[ \frac{\partial}{\partial \Sigma_k} \log|\Sigma_k| + \frac{\partial}{\partial \Sigma_k}(\xx_i - \mub_k)^\T \Sigma_k^{-1} (\xx_i - \mub_k) \right] \\
    &= \sum_i r_{ik} \left[ \frac{\partial}{\partial \Sigma_k} \log|\Sigma_k| + (\xx_i - \mub_k) (\xx_i - \mub_k)^\T \right] \text{ Matrix Cookbook 72} \\
    &= \sum_i r_{ik} \left[ -\Sigma_k + (\xx_i - \mub_k) (\xx_i - \mub_k)^\T \right] \text{ Math stack exchange} \\
    \Sigma_k \sum_i r_{ik} &= \sum_i r_{ik} (\xx_i - \mub_k) (\xx_i - \mub_k)^\T\\
    \Sigma_k &= \frac{ \sum_i r_{ik} (\xx_i - \mub_k) (\xx_i - \mub_k)^\T}{r_k} \text{ Desired form 1}\\
    &= \frac{1}{r_k} \sum_i r_{ik} \left[\xx_i\xx_i^\T  - \xx_i \mub_k ^\T - \mub_k \xx_i^\T + \mub_k\mub_k^\T \right] \\
    &= \frac{1}{r_k} \sum_i r_{ik} \left[\xx_i\xx_i^\T - \mub_k\mub_k^\T \right] \\
    &= \frac{1}{r_k} \sum_i r_{ik} \xx_i\xx_i^\T - \frac{1}{r_k} \sum_i r_{ik} \mub_k\mub_k^\T \\
    &= \frac{1}{r_k} \sum_i r_{ik} \xx_i\xx_i^\T - \frac{r_k}{r_k} \sum_i \mub_k\mub_k^\T \\
    &= \frac{1}{r_k} \sum_i r_{ik} \xx_i\xx_i^\T -  \sum_i \mub_k\mub_k^\T \text{ As desired.}
    \end{align*}

\newpage


\textbf{2 (Murphy 11.3 - EM for Mixtures of Bernoullis)} Show that the M step for ML estimation
of a mixture of Bernoullis is given by
\[
    \mu_{kj} = \frac{\sum_i r_{ik}x_{ij}}{\sum_i r_{ik}}.
\]
Show that the M step for MAP estimation of a mixture of Bernoullis with a $\beta(\alpha,\beta)$ prior
is given by
\[
    \mu_{kj} = \frac{\left(\sum_i r_{ik}x_{ij}\right) + \alpha - 1}{\left(\sum_i r_{ik}\right) + \alpha + \beta - 2}.
\]

\textbf{3 (MAP Mixture of Gaussians)} Consider a mixture of Gaussians with a Dirichlet prior on the
mixture weights $\pib \sim \mathrm{Dir}(\alphab)$ and a Negative Inverse Walshart prior on the
mean and covariance within each class
$\mub_k,\Sigmab_k \sim \mathrm{NIW}(\mathbf{m}_0,\kappa_0,\nu_0,\mathbf{S}_0)$ with $\kappa_0=0$ so
only the covariance matrices are regularized. Use $\mathbf{S}_0 = \mathrm{diag}(s_1^2,\dots,s_D^2)/K^{1/D}$
where $s_j = \sum_i(x_{ij} - \overline{x}_j)^2/N$ is the pooled variance in dimension $j$. Use $\nu_0 = D + 2$,
as that is the weakest prior that is still proper. Use $\alphab = \1$. This is all detailed in Murphy 11.4.2.8. Download the
wine quality data at \url{https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv}
and \url{https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv}. Pool both
red and white wine datasets into one dataset and cluster this data using a 2 component MAP Gaussian mixture model
with the EM algorithm. Do the clusters roughly correspond to the color of the wine \{white,red\} (back this with
numbers)? Provide a convergence plot of the MAP objective. If it doesn't monotonically increase there is a bug
in your code or math.

\textbf{4 (MAP Mixture of Bernoullis)} Consider a mixture of Bernoullis with a Dirichlet prior on the mixture
weights $\pib \sim \mathrm{Dir}(\alphab)$ and a Beta prior on the mean parameter $\mu_{kj} \sim \beta(\alpha,\beta)$.
Use $\alphab=\1$ and choose an appropriate $(\alpha,\beta)$ pair for your prior (back this up).
Note that the M step for the mean is given in problem 2 (Murphy 11.3). Cluster the MNIST training dataset we used
from homework 1 (\url{http://pjreddie.com/media/files/mnist_train.csv}) using this mixture with 10 components.
Provide a convergence plot of the MAP objective
(which must monotonically increase) and plot the mean images for each component. Do the clusters roughly correspond to
different digits (back this up somehow)?

\textbf{5 (Operations Preserving Kernels)} Let $\kappa(\cdot,\cdot)$ and $\lambda(\cdot,\cdot)$ be valid positive
semi-definite (Mercer) kernels mapping from a sample space $\mathcal{S}$ to $\RR$. Let $\alpha \geq 0$ be a real number
and let $x$ and $y$ be elements of $\mathcal{S}$. Prove that
\begin{enumerate}[(a)]
    \item $\alpha\kappa(x,y)$ is a valid kernel.
    \item $\kappa(x,y) + \lambda(x,y)$ is a valid kernel.
    \item $\kappa(x,y)\lambda(x,y)$ is a valid kernel. \textit{Hint:} consider the Cholesky decomposition
        of the corresponding covariance matrix generated by the product of the kernels.
    \item $p(\kappa(x,y))$ is a valid kernel where $p(\cdot)$ is a polynomial
        with non-negative coefficients.
    \item $\exp(\kappa(x,y))$ is a valid kernel.
    \item $f(x)\kappa(x,y)f(y)$ for all $f : \mathcal{S}\to\RR$.
\end{enumerate}
To prove these, the main method would be to consider an arbitrary covariance matrix
generated by these kernels and assert that the conditions on this covariance matrix (being
symmetric and positive semi-definite) still hold.

\textbf{6 (Totally Optional Extra Credit)} Prove (a), (b), and (c) from problem 5 (above) for the special
case of stationary kernels $\kappa(x,y) = \kappa(x-y)$ using Bochner's Theorem and properties of
Fourier transforms. Note that for (c) the Convolution Theorem might be helpful.

\end{document}
